{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b3dda6f",
   "metadata": {},
   "source": [
    "# Arabic NLP Project – KALIMAT Dataset\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#1-introduction)\n",
    "2. [Data Exploration and Preprocessing](#2-data-exploration-and-preprocessing)\n",
    "3. [Phase 1: Traditional Approaches](#3-phase-1-traditional-approaches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aadd371",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7754f2e0",
   "metadata": {},
   "source": [
    "### Project overview and objectives\n",
    "\n",
    "This project explores two core NLP tasks on Arabic text data: text classification and text summarization. The goal is to evaluate and compare traditional machine learning methods and modern deep learning/transformer-based models in the context of Arabic language processing. The project aims to highlight how each approach handles Arabic-specific linguistic challenges and determine the most effective techniques for each task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9358125",
   "metadata": {},
   "source": [
    "### Description of the chosen dataset and tasks\n",
    "\n",
    "The dataset used in this project is the **KALIMAT Multipurpose Arabic Corpus**, which contains Arabic newspaper articles across six topics: culture, economy, local news, international news, religion, and sports. \n",
    "\n",
    "The two NLP tasks applied to this dataset are:\n",
    "1. **Text Classification** — categorizing articles based on their topical class.\n",
    "2. **Text Summarization** — generating concise summaries of Arabic news articles using extractive and abstractive methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837c9809",
   "metadata": {},
   "source": [
    "### Brief background on relevant challenges (Arabic NLP or multimodal processing)\n",
    "\n",
    "Arabic NLP presents unique challenges due to the language’s complex morphology, rich inflectional structure, varying dialects, and orthographic variations (e.g., different forms of the same letter). Additionally, Arabic lacks large-scale annotated resources compared to English, making both training and evaluation more difficult. Handling diacritics, normalization, tokenization, and stemming are essential yet non-trivial preprocessing steps in any Arabic NLP pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ba4767",
   "metadata": {},
   "source": [
    "## 2. Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0c2ae6",
   "metadata": {},
   "source": [
    "### Dataset Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90f3c540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load Kalimat corpus into a dataframe. Show counts per category, example texts, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415e9eb0",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb667348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - Normalize Arabic letters\n",
    "# - Remove diacritics\n",
    "# - Remove stopwords\n",
    "# - Apply stemming (ISRI or whatever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a85547b",
   "metadata": {},
   "source": [
    "### Data Visualization and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec780ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize class distribution (bar plot), maybe word frequencies, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59a9d7e",
   "metadata": {},
   "source": [
    "## 3. Phase 1: Traditional Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521ef161",
   "metadata": {},
   "source": [
    "### Traditional implementation of Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "002b515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - Apply TF-IDF or BoW\n",
    "# - Train classifiers (Naive Bayes, SVM)\n",
    "# - Evaluate with F1-score, confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4204935d",
   "metadata": {},
   "source": [
    "### Traditional implementation of Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6340c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO :\n",
    "# - Use extractive summarization (e.g., TF-IDF sentence ranking, LexRank)\n",
    "# - Display summary examples\n",
    "# - Evaluate with ROUGE score (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb50360",
   "metadata": {},
   "source": [
    "### Evaluation and results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378f2c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  \n",
    "# - Evaluate model performance for both tasks.  \n",
    "# - Discuss results and observed strengths/weaknesses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
